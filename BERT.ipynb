{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a7W_6VdC1XD9","executionInfo":{"status":"ok","timestamp":1606448231230,"user_tz":480,"elapsed":2498,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}},"outputId":"3215c30e-3b71-4b25-d1c3-1cbde45dc8e6"},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n","Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VvGjmTgp2E9Y","executionInfo":{"status":"ok","timestamp":1606448233422,"user_tz":480,"elapsed":4680,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["import os\n","import time\n","import random\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.metrics import roc_curve, auc\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","from transformers import DistilBertForSequenceClassification\n","from transformers import AdamW\n","from transformers import DistilBertTokenizerFast"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"hGh5augPFKxC","executionInfo":{"status":"ok","timestamp":1606448233423,"user_tz":480,"elapsed":4675,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["RANDOM_SEED = 2020\n","torch.manual_seed(RANDOM_SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","np.random.seed(RANDOM_SEED)\n","random.seed(RANDOM_SEED)\n","\n","DATA_PATH = \"/content/\""],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XULJ8LvJvw50"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"o5x7fDFr2EF3","executionInfo":{"status":"ok","timestamp":1606448233424,"user_tz":480,"elapsed":4671,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["class CustomDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"7XQ6A2SP0tDu","executionInfo":{"status":"ok","timestamp":1606448234072,"user_tz":480,"elapsed":5314,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n","\n","train_df = pd.read_csv(os.path.join(DATA_PATH, \"sat_train.tsv\"), sep=\"\\t\")\n","valid_df = pd.read_csv(os.path.join(DATA_PATH, \"sat_valid.tsv\"), sep=\"\\t\")\n","test_df = pd.read_csv(os.path.join(DATA_PATH, \"sat_test.tsv\"), sep=\"\\t\")\n","\n","train_encodings = tokenizer(train_df[\"context\"].values.tolist(), truncation=True, padding=True)\n","valid_encodings = tokenizer(valid_df[\"context\"].values.tolist(), truncation=True, padding=True)\n","test_encodings = tokenizer(test_df[\"context\"].values.tolist(), truncation=True, padding=True)\n","\n","train_dataset = CustomDataset(train_encodings, train_df[\"label\"].values)\n","valid_dataset = CustomDataset(valid_encodings, valid_df[\"label\"].values)\n","test_dataset = CustomDataset(test_encodings, test_df[\"label\"].values)\n","\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x_s7DV-Bvw51"},"source":["## Test function"]},{"cell_type":"code","metadata":{"id":"w5Y-N7XB49C_","executionInfo":{"status":"ok","timestamp":1606448234073,"user_tz":480,"elapsed":5310,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["def train(model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer, device: str):\n","    model.train()\n","\n","    epoch_loss = 0\n","\n","    for _, batch in enumerate(loader):\n","        optimizer.zero_grad()\n","\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs[0]\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","\n","    return epoch_loss / len(loader)\n","\n","\n","def evaluate(model: nn.Module, loader: DataLoader, device: str):\n","    model.eval()\n","    epoch_loss = 0\n","    with torch.no_grad():\n","        for _, batch in enumerate(loader):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs[0]\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(loader)\n","\n","\n","def test(model: nn.Module, loader: DataLoader, device: str):\n","\n","    with torch.no_grad():\n","        y_real = []\n","        y_pred = []\n","        model.eval()\n","\n","        for batch in loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            output = model(input_ids, attention_mask=attention_mask)[0]\n","            y_pred += [output.cpu()]\n","            y_real += [batch[\"labels\"]]\n","            \n","        y_real = torch.cat(y_real)\n","        y_pred = torch.cat(y_pred)[:,1]\n","\n","    fpr, tpr, _ = roc_curve(y_real, y_pred)\n","    auroc = auc(fpr, tpr)\n","\n","    return auroc\n","\n","\n","def epoch_time(start_time: int, end_time: int):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TkZBqv25vw51"},"source":["## Before fine-tuning"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XrLWbTRBCvUV","executionInfo":{"status":"ok","timestamp":1606448243660,"user_tz":480,"elapsed":14891,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}},"outputId":"689d7de3-74d2-46b8-9368-42ae49ad8698"},"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","before_tuning_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n","model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n","_ = model.to(device)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"ir4wVDHGvw51"},"source":["## Fine tuning"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rRe7fiWY3cLp","executionInfo":{"status":"ok","timestamp":1606448258633,"user_tz":480,"elapsed":29857,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}},"outputId":"e904131e-0ee5-437b-d9ee-073435a55336"},"source":["N_EPOCHS = 20\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","\n","    train_loss = train(model, train_loader, optimizer, device)\n","    valid_loss = evaluate(model, valid_loader, device)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n","    print(f\"\\tTrain Loss: {train_loss:.5f}\")\n","    print(f\"\\t Val. Loss: {valid_loss:.5f}\")\n","\n","test_auroc = test(model, test_loader, device)\n","print(f'| Test AUROC: {test_auroc:.5f}')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Time: 0m 0s\n","\tTrain Loss: 0.53651\n","\t Val. Loss: 0.36603\n","Epoch: 02 | Time: 0m 0s\n","\tTrain Loss: 0.41014\n","\t Val. Loss: 0.34873\n","Epoch: 03 | Time: 0m 0s\n","\tTrain Loss: 0.32635\n","\t Val. Loss: 0.44384\n","Epoch: 04 | Time: 0m 0s\n","\tTrain Loss: 0.26509\n","\t Val. Loss: 0.56301\n","Epoch: 05 | Time: 0m 0s\n","\tTrain Loss: 0.11553\n","\t Val. Loss: 0.37966\n","Epoch: 06 | Time: 0m 0s\n","\tTrain Loss: 0.13106\n","\t Val. Loss: 0.35931\n","Epoch: 07 | Time: 0m 0s\n","\tTrain Loss: 0.09925\n","\t Val. Loss: 0.37978\n","Epoch: 08 | Time: 0m 0s\n","\tTrain Loss: 0.10690\n","\t Val. Loss: 0.53429\n","Epoch: 09 | Time: 0m 0s\n","\tTrain Loss: 0.08450\n","\t Val. Loss: 0.33815\n","Epoch: 10 | Time: 0m 0s\n","\tTrain Loss: 0.06087\n","\t Val. Loss: 0.39603\n","Epoch: 11 | Time: 0m 0s\n","\tTrain Loss: 0.04214\n","\t Val. Loss: 0.37322\n","Epoch: 12 | Time: 0m 0s\n","\tTrain Loss: 0.03168\n","\t Val. Loss: 0.39570\n","Epoch: 13 | Time: 0m 0s\n","\tTrain Loss: 0.02711\n","\t Val. Loss: 1.00184\n","Epoch: 14 | Time: 0m 0s\n","\tTrain Loss: 0.02792\n","\t Val. Loss: 1.09086\n","Epoch: 15 | Time: 0m 0s\n","\tTrain Loss: 0.02373\n","\t Val. Loss: 1.27300\n","Epoch: 16 | Time: 0m 0s\n","\tTrain Loss: 0.08330\n","\t Val. Loss: 0.39793\n","Epoch: 17 | Time: 0m 0s\n","\tTrain Loss: 0.02993\n","\t Val. Loss: 0.32387\n","Epoch: 18 | Time: 0m 0s\n","\tTrain Loss: 0.02153\n","\t Val. Loss: 0.45724\n","Epoch: 19 | Time: 0m 0s\n","\tTrain Loss: 0.01388\n","\t Val. Loss: 0.45638\n","Epoch: 20 | Time: 0m 0s\n","\tTrain Loss: 0.01161\n","\t Val. Loss: 0.48909\n","| Test AUROC: 0.84615\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"js_bb7zrwn5n","executionInfo":{"status":"ok","timestamp":1606448260796,"user_tz":480,"elapsed":32013,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}},"outputId":"d1eab526-a298-4dc1-b78e-d79726a6deed"},"source":["_ = model.cpu()\n","\n","before_test_auroc = test(before_tuning_model, test_loader, \"cpu\")\n","test_auroc = test(model, test_loader, \"cpu\")\n","\n","print(f'SAT Dataset Test AUROC: {before_test_auroc:.5f}')\n","print(f'SAT Dataset Test AUROC: {test_auroc:.5f}')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["SAT Dataset Test AUROC: 0.76923\n","SAT Dataset Test AUROC: 0.84615\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YJrvNRHmvPyt","executionInfo":{"status":"ok","timestamp":1606448260797,"user_tz":480,"elapsed":32008,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["def predict_problem(model, problem, device):\n","    sat_encodings = [tokenizer(sentence) for sentence in problem]\n","    with torch.no_grad():\n","        outputs = []\n","        for sat_encoding in sat_encodings:\n","            input_ids = torch.LongTensor([sat_encoding[\"input_ids\"]]).to(device)\n","            attention_mask = torch.LongTensor(sat_encoding[\"attention_mask\"]).to(device)\n","            output = model(input_ids, attention_mask=attention_mask)\n","            outputs += [output[0]]\n","        output = torch.cat(outputs)[:,1]\n","    return output.tolist()\n","\n","\n","def predict_problem_with_models(model_list, problem):\n","    scores = {}\n","    for model_name, classifier in model_list:\n","        score = predict_problem(classifier, problem, \"cpu\")\n","        scores[model_name] = score\n","\n","    score_df = pd.DataFrame(scores).T\n","    score_df.columns = [f\"answer_{i}_score\" for i in range(1,6)]\n","    selected_answer = pd.Series(np.argmin(score_df.values, 1)+1, index=score_df.index, name=\"selected_answer\")\n","    return pd.concat([selected_answer, score_df], 1)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"4BTlT2n_xc7J","executionInfo":{"status":"ok","timestamp":1606448260797,"user_tz":480,"elapsed":32006,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["model_list = [\n","    (\"before_tuning_BERT\", before_tuning_model),\n","    (\"after_tuning_BERT\", model),\n","]"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"6jmIaEqtwU8P","executionInfo":{"status":"ok","timestamp":1606448260798,"user_tz":480,"elapsed":32001,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["problem_1 = [ \n","    \"Competitive activities can be more than just performance showcases which the best is recognized and the rest are overlooked.\",\n","    \"The provision of timely, constructive feedback to participants on performance is an asset that some competitions and contests offer.\",\n","    \"The provision of that type of feedback can be interpreted as shifting the emphasis to demonstrating superior performance but not necessarily excellence.\",\n","    \"The emphasis on superiority is what we typically see as fostering a detrimental effect of competition.\",\n","    \"Information about performance can be very helpful, not only to the participant who does not win or place but also to those who do.\",\n","]\n","problem_1_label = [0, 1, 1, 1, 1]"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":111},"id":"VWc9WRc5wXWD","executionInfo":{"status":"ok","timestamp":1606448261265,"user_tz":480,"elapsed":32461,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}},"outputId":"016f706f-e0fd-42eb-b32e-0cb0383d7bc7"},"source":["predict_problem_with_models(model_list, problem_1)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>selected_answer</th>\n","      <th>answer_1_score</th>\n","      <th>answer_2_score</th>\n","      <th>answer_3_score</th>\n","      <th>answer_4_score</th>\n","      <th>answer_5_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>before_tuning_BERT</th>\n","      <td>1</td>\n","      <td>0.138504</td>\n","      <td>0.169909</td>\n","      <td>0.157586</td>\n","      <td>0.144427</td>\n","      <td>0.157007</td>\n","    </tr>\n","    <tr>\n","      <th>after_tuning_BERT</th>\n","      <td>1</td>\n","      <td>0.199188</td>\n","      <td>3.379380</td>\n","      <td>3.499100</td>\n","      <td>3.511260</td>\n","      <td>3.327176</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    selected_answer  ...  answer_5_score\n","before_tuning_BERT                1  ...        0.157007\n","after_tuning_BERT                 1  ...        3.327176\n","\n","[2 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"L1SdmVAsxbfY","executionInfo":{"status":"ok","timestamp":1606448261266,"user_tz":480,"elapsed":32459,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["problem_2 = [ \n","    \"People from more individualistic cultural contexts tend to be motivated to maintain self-focused agency or control 1 as these serve as the basis of one’s self-worth.\",\n","    \"With this form of agency comes the belief that individual successes 2 depending primarily on one’s own abilities and actions, and thus, whether by influencing the environment or trying to accept one’s circumstances, the use of control ultimately centers on the individual.\",\n","    \"The independent self may be more 3 driven to cope by appealing to a sense of agency or control.\",\n","    \"Research has shown 4 that East Asians prefer to receive, but not seek, more social support rather than seek personal control in certain cases.\",\n","    \"Therefore, people 5 who hold a more interdependent self-construal may prefer to cope in a way that promotes harmony in relationships.\",\n","]\n","problem_2_label = [1, 0, 1, 1, 1]"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":111},"id":"RsFKCrhVuyBf","executionInfo":{"status":"ok","timestamp":1606448262149,"user_tz":480,"elapsed":33334,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}},"outputId":"89458768-182f-4606-ff6d-8801927cf240"},"source":["predict_problem_with_models(model_list, problem_2)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>selected_answer</th>\n","      <th>answer_1_score</th>\n","      <th>answer_2_score</th>\n","      <th>answer_3_score</th>\n","      <th>answer_4_score</th>\n","      <th>answer_5_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>before_tuning_BERT</th>\n","      <td>5</td>\n","      <td>0.122632</td>\n","      <td>0.116739</td>\n","      <td>0.130244</td>\n","      <td>0.120807</td>\n","      <td>0.113969</td>\n","    </tr>\n","    <tr>\n","      <th>after_tuning_BERT</th>\n","      <td>3</td>\n","      <td>3.455905</td>\n","      <td>3.372821</td>\n","      <td>2.742554</td>\n","      <td>3.573965</td>\n","      <td>3.516525</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    selected_answer  ...  answer_5_score\n","before_tuning_BERT                5  ...        0.113969\n","after_tuning_BERT                 3  ...        3.516525\n","\n","[2 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":15}]}]}