{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"baseline_pre_training.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WEiWqpvJzG22","executionInfo":{"status":"ok","timestamp":1606111973738,"user_tz":480,"elapsed":1418,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}},"outputId":"28d02983-f0fc-4e8c-f4cc-86cc30e41aa1"},"source":["from copy import deepcopy\n","import time\n","import random\n","import numpy as np\n","from sklearn.metrics import roc_curve, auc\n","\n","import nltk\n","\n","nltk.download(\"punkt\")\n","from nltk.tokenize import word_tokenize\n","\n","import torch\n","import torch.nn as nn\n","\n","from torchtext.data import Field\n","from torchtext.data import TabularDataset\n","from torchtext.data import BucketIterator\n","from torchtext.data import Iterator"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DXvvT31XzRa4","executionInfo":{"status":"ok","timestamp":1606111973738,"user_tz":480,"elapsed":1408,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["RANDOM_SEED = 2020\n","torch.manual_seed(RANDOM_SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","np.random.seed(RANDOM_SEED)\n","random.seed(RANDOM_SEED)\n","\n","# DATA_PATH = \"data/processed/\"\n","DATA_PATH = \"/content/\""],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EvjX2S_WkRF8"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"k8Twwhw9zPdJ","executionInfo":{"status":"ok","timestamp":1606111974974,"user_tz":480,"elapsed":2638,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["TEXT = Field(\n","    sequential=True,\n","    use_vocab=True,\n","    tokenize=word_tokenize,\n","    lower=True,\n","    batch_first=True,\n",")\n","LABEL = Field(\n","    sequential=False,\n","    use_vocab=False,\n","    batch_first=True,\n",")\n","\n","\n","cola_train_data, cola_valid_data, cola_test_data = TabularDataset.splits(\n","    path=DATA_PATH,\n","    train=\"cola_train.tsv\",\n","    validation=\"cola_valid.tsv\",\n","    test=\"cola_test.tsv\",\n","    format=\"tsv\",\n","    fields=[(\"text\", TEXT), (\"label\", LABEL)],\n","    skip_header=1,\n",")\n","\n","TEXT.build_vocab(cola_train_data, min_freq=2)\n","\n","\n","cola_train_iterator, cola_valid_iterator, cola_test_iterator = BucketIterator.splits(\n","    (cola_train_data, cola_valid_data, cola_test_data),\n","    batch_size=32,\n","    device=None,\n","    sort=False,\n",")\n","\n","\n","sat_train_data, sat_valid_data, sat_test_data = TabularDataset.splits(\n","    path=DATA_PATH,\n","    train=\"sat_train.tsv\",\n","    validation=\"sat_valid.tsv\",\n","    test=\"sat_test.tsv\",\n","    format=\"tsv\",\n","    fields=[(\"text\", TEXT), (\"label\", LABEL)],\n","    skip_header=1,\n",")\n","\n","sat_train_iterator, sat_valid_iterator, sat_test_iterator = BucketIterator.splits(\n","    (sat_train_data, sat_valid_data, sat_test_data),\n","    batch_size=8,\n","    device=None,\n","    sort=False,\n",")"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NaBRroeFkRF8"},"source":["## LSTM Classifier"]},{"cell_type":"code","metadata":{"id":"-ycMzmLHzbI9","executionInfo":{"status":"ok","timestamp":1606111974974,"user_tz":480,"elapsed":2632,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["class LSTMClassifier(nn.Module):\n","    def __init__(self, num_embeddings, embedding_dim, hidden_size, num_layers, pad_idx):\n","        super().__init__()\n","        self.embed_layer = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=pad_idx)\n","        self.lstm_layer = nn.LSTM(\n","            input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, dropout=0.5\n","        )\n","        self.last_layer = nn.Sequential(\n","            nn.Linear(hidden_size * 2, hidden_size),\n","            nn.Dropout(0.5),\n","            nn.LeakyReLU(),\n","            nn.Linear(hidden_size, 1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        embed_x = self.embed_layer(x)\n","        output, (_, _) = self.lstm_layer(embed_x)\n","        last_output = output[:, -1, :]\n","        last_output = self.last_layer(last_output)\n","        return last_output"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"xHX9pXijzdKM","executionInfo":{"status":"ok","timestamp":1606111974975,"user_tz":480,"elapsed":2628,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["def train(model: nn.Module, iterator: Iterator, optimizer: torch.optim.Optimizer, criterion: nn.Module, device: str):\n","    model.train()\n","\n","    epoch_loss = 0\n","\n","    for _, batch in enumerate(iterator):\n","        optimizer.zero_grad()\n","\n","        text = batch.text\n","        if text.shape[0] > 1:\n","            label = batch.label.type(torch.FloatTensor)\n","            text = text.to(device)\n","            label = label.to(device)\n","\n","            output = model(text).flatten()\n","            loss = criterion(output, label)\n","            loss.backward()\n","\n","            optimizer.step()\n","\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)\n","\n","\n","def evaluate(model: nn.Module, iterator: Iterator, criterion: nn.Module, device: str):\n","    model.eval()\n","    epoch_loss = 0\n","    with torch.no_grad():\n","        for _, batch in enumerate(iterator):\n","            text = batch.text\n","            label = batch.label.type(torch.FloatTensor)\n","            text = text.to(device)\n","            label = label.to(device)\n","            output = model(text).flatten()\n","            loss = criterion(output, label)\n","\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)\n","\n","\n","def test(model: nn.Module, iterator: Iterator, device: str):\n","\n","    with torch.no_grad():\n","        y_real = []\n","        y_pred = []\n","        model.eval()\n","        for batch in iterator:\n","            text = batch.text\n","            label = batch.label.type(torch.FloatTensor)\n","            text = text.to(device)\n","\n","            output = model(text).flatten().cpu()\n","\n","            y_real += [label]\n","            y_pred += [output]\n","\n","        y_real = torch.cat(y_real)\n","        y_pred = torch.cat(y_pred)\n","\n","    fpr, tpr, _ = roc_curve(y_real, y_pred)\n","    auroc = auc(fpr, tpr)\n","\n","    return auroc\n","\n","\n","def epoch_time(start_time: int, end_time: int):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nqYq9AypkRF9"},"source":["## Pretrain with cola dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"so3L6wyQzeEI","executionInfo":{"status":"ok","timestamp":1606112099024,"user_tz":480,"elapsed":126670,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}},"outputId":"68f3b334-406c-48b1-f680-651a0539dc85"},"source":["PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","N_EPOCHS = 20\n","\n","lstm_classifier = LSTMClassifier(\n","    num_embeddings=len(TEXT.vocab),\n","    embedding_dim=100,\n","    hidden_size=200,\n","    num_layers=4,\n","    pad_idx=PAD_IDX,\n",")\n","if torch.cuda.is_available():\n","    device = \"cuda:0\"\n","else:\n","    device = \"cpu\"\n","_ = lstm_classifier.to(device)\n","\n","optimizer = torch.optim.Adam(lstm_classifier.parameters())\n","bce_loss_fn = nn.BCELoss()\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","\n","    train_loss = train(lstm_classifier, cola_train_iterator, optimizer, bce_loss_fn, device)\n","    valid_loss = evaluate(lstm_classifier, cola_valid_iterator, bce_loss_fn, device)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n","    print(f\"\\tTrain Loss: {train_loss:.5f}\")\n","    print(f\"\\t Val. Loss: {valid_loss:.5f}\")\n","\n","test_auroc = test(lstm_classifier, cola_test_iterator, device)\n","\n","print(f\"| CoLA Dataset Test AUROC: {test_auroc:.5f}\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Time: 0m 6s\n","\tTrain Loss: 0.61555\n","\t Val. Loss: 0.61686\n","Epoch: 02 | Time: 0m 5s\n","\tTrain Loss: 0.61094\n","\t Val. Loss: 0.61838\n","Epoch: 03 | Time: 0m 5s\n","\tTrain Loss: 0.61013\n","\t Val. Loss: 0.61736\n","Epoch: 04 | Time: 0m 6s\n","\tTrain Loss: 0.63810\n","\t Val. Loss: 0.61793\n","Epoch: 05 | Time: 0m 6s\n","\tTrain Loss: 0.61023\n","\t Val. Loss: 0.61750\n","Epoch: 06 | Time: 0m 5s\n","\tTrain Loss: 0.61001\n","\t Val. Loss: 0.61782\n","Epoch: 07 | Time: 0m 5s\n","\tTrain Loss: 0.60866\n","\t Val. Loss: 0.61823\n","Epoch: 08 | Time: 0m 6s\n","\tTrain Loss: 0.61012\n","\t Val. Loss: 0.61762\n","Epoch: 09 | Time: 0m 5s\n","\tTrain Loss: 0.60868\n","\t Val. Loss: 0.61912\n","Epoch: 10 | Time: 0m 5s\n","\tTrain Loss: 0.60851\n","\t Val. Loss: 0.61802\n","Epoch: 11 | Time: 0m 5s\n","\tTrain Loss: 0.60906\n","\t Val. Loss: 0.61824\n","Epoch: 12 | Time: 0m 6s\n","\tTrain Loss: 0.60837\n","\t Val. Loss: 0.63035\n","Epoch: 13 | Time: 0m 6s\n","\tTrain Loss: 0.60908\n","\t Val. Loss: 0.61876\n","Epoch: 14 | Time: 0m 6s\n","\tTrain Loss: 0.60899\n","\t Val. Loss: 0.61721\n","Epoch: 15 | Time: 0m 6s\n","\tTrain Loss: 0.60798\n","\t Val. Loss: 0.61769\n","Epoch: 16 | Time: 0m 6s\n","\tTrain Loss: 0.60839\n","\t Val. Loss: 0.61835\n","Epoch: 17 | Time: 0m 6s\n","\tTrain Loss: 0.60854\n","\t Val. Loss: 0.61792\n","Epoch: 18 | Time: 0m 6s\n","\tTrain Loss: 0.60846\n","\t Val. Loss: 0.61733\n","Epoch: 19 | Time: 0m 6s\n","\tTrain Loss: 0.60844\n","\t Val. Loss: 0.61763\n","Epoch: 20 | Time: 0m 6s\n","\tTrain Loss: 0.60821\n","\t Val. Loss: 0.61739\n","| CoLA Dataset Test AUROC: 0.45410\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y_-a20XpdJZx","executionInfo":{"status":"ok","timestamp":1606112099025,"user_tz":480,"elapsed":126664,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}},"outputId":"a7cd1718-d4a9-4a30-8383-ec6c4f1241a8"},"source":["before_tuning_lstm_classifier = deepcopy(lstm_classifier)\n","\n","lstm_sat_test_auroc = test(lstm_classifier, sat_test_iterator, device)\n","print(f'| SAT Dataset Test AUROC: {lstm_sat_test_auroc:.5f}')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["| SAT Dataset Test AUROC: 0.32828\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"laz-_3zIkRF9"},"source":["## Fine Tuning"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WG9ulCznWHZ8","executionInfo":{"status":"ok","timestamp":1606112104822,"user_tz":480,"elapsed":132453,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}},"outputId":"b6253b3d-c63d-4b20-c301-2714b50b7612"},"source":["PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","N_EPOCHS = 20\n","\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","\n","    train_loss = train(lstm_classifier, sat_train_iterator, optimizer, bce_loss_fn, device)\n","    valid_loss = evaluate(lstm_classifier, sat_valid_iterator, bce_loss_fn, device)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n","    print(f\"\\tTrain Loss: {train_loss:.5f}\")\n","    print(f\"\\t Val. Loss: {valid_loss:.5f}\")\n","\n","lstm_tuned_test_auroc = test(lstm_classifier, sat_test_iterator, device)\n","\n","print(f\"| SAT Dataset Test AUROC: {lstm_tuned_test_auroc:.5f}\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Time: 0m 0s\n","\tTrain Loss: 0.48172\n","\t Val. Loss: 0.50492\n","Epoch: 02 | Time: 0m 0s\n","\tTrain Loss: 0.46107\n","\t Val. Loss: 0.50227\n","Epoch: 03 | Time: 0m 0s\n","\tTrain Loss: 0.45895\n","\t Val. Loss: 0.49783\n","Epoch: 04 | Time: 0m 0s\n","\tTrain Loss: 0.46671\n","\t Val. Loss: 0.50096\n","Epoch: 05 | Time: 0m 0s\n","\tTrain Loss: 0.47465\n","\t Val. Loss: 0.50480\n","Epoch: 06 | Time: 0m 0s\n","\tTrain Loss: 0.46005\n","\t Val. Loss: 0.50860\n","Epoch: 07 | Time: 0m 0s\n","\tTrain Loss: 0.46610\n","\t Val. Loss: 0.50514\n","Epoch: 08 | Time: 0m 0s\n","\tTrain Loss: 0.46076\n","\t Val. Loss: 0.50537\n","Epoch: 09 | Time: 0m 0s\n","\tTrain Loss: 0.47032\n","\t Val. Loss: 0.50433\n","Epoch: 10 | Time: 0m 0s\n","\tTrain Loss: 0.46195\n","\t Val. Loss: 0.50492\n","Epoch: 11 | Time: 0m 0s\n","\tTrain Loss: 0.46063\n","\t Val. Loss: 0.50391\n","Epoch: 12 | Time: 0m 0s\n","\tTrain Loss: 0.46275\n","\t Val. Loss: 0.50435\n","Epoch: 13 | Time: 0m 0s\n","\tTrain Loss: 0.46059\n","\t Val. Loss: 0.49953\n","Epoch: 14 | Time: 0m 0s\n","\tTrain Loss: 0.46787\n","\t Val. Loss: 0.50112\n","Epoch: 15 | Time: 0m 0s\n","\tTrain Loss: 0.46817\n","\t Val. Loss: 0.49692\n","Epoch: 16 | Time: 0m 0s\n","\tTrain Loss: 0.45853\n","\t Val. Loss: 0.49599\n","Epoch: 17 | Time: 0m 0s\n","\tTrain Loss: 0.44751\n","\t Val. Loss: 0.49316\n","Epoch: 18 | Time: 0m 0s\n","\tTrain Loss: 0.46987\n","\t Val. Loss: 0.49438\n","Epoch: 19 | Time: 0m 0s\n","\tTrain Loss: 0.46740\n","\t Val. Loss: 0.49149\n","Epoch: 20 | Time: 0m 0s\n","\tTrain Loss: 0.46930\n","\t Val. Loss: 0.48892\n","| SAT Dataset Test AUROC: 0.46465\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4PjKb-tcllKb","executionInfo":{"status":"ok","timestamp":1606112104823,"user_tz":480,"elapsed":132447,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}},"outputId":"719720d7-c7b1-4126-e62c-a9e5a99ace53"},"source":["print(f\"Before fine-tuning SAT Dataset Test AUROC: {lstm_sat_test_auroc:.5f}\")\n","print(f\"After fine-tuning SAT Dataset Test AUROC: {lstm_tuned_test_auroc:.5f}\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Before fine-tuning SAT Dataset Test AUROC: 0.32828\n","After fine-tuning SAT Dataset Test AUROC: 0.46465\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R6mqhIBBnJqz","executionInfo":{"status":"ok","timestamp":1606112274950,"user_tz":480,"elapsed":463,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}}},"source":["def demo(classifier, device):\n","    sat_test = [ \n","        \"Speculations about the meaning and purpose of prehistoric art [rely] heavily on analogies drawn with modern-day hunter-gatherer societies.\",\n","        \"Such primitive societies, [as] Steven Mithen emphasizes in The Prehistory of the Modern Mind, tend to view man and beast, animal and plant, organic and inorganic spheres, as participants in an integrated, animated totality.\",\n","        \"The dual expressions of this tendency are anthropomorphism (the practice of regarding animals as humans) and totemism (the practice of regarding humans as animals), both of [which] spread through the visual art and the mythology of primitive cultures.\",\n","        \"When considered in this light, the visual preoccupation of early humans with the nonhuman creatures [inhabited] their world becomes profoundly meaningful.\",\n","        \"In the practice of totemism, he has suggested, an unlettered humanity “broods upon [itself] and its place in nature.”\",\n","    ]\n","    sat_label = [1, 1, 1, 0, 1]\n","    sat_test = list(map(lambda x: x.replace(\"[\", \"\").replace(\"]\", \"\"), sat_test))\n","    tokenized_sentences = [word_tokenize(sentence) for sentence in sat_test]\n","    sentences = []\n","    for tokenized_sentence in tokenized_sentences:\n","        sentences.append([TEXT.vocab.stoi[word] for word in tokenized_sentence])\n","\n","    classifier.eval()\n","    predict = []\n","    for sentence in sentences:\n","        sentence = torch.LongTensor([sentence])\n","        predict += [classifier(sentence.to(device)).item()]\n","    return predict"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"INn0wsmvnLF_","executionInfo":{"status":"ok","timestamp":1606112275872,"user_tz":480,"elapsed":408,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}},"outputId":"06ed7ffa-1700-4296-b489-34ba24d49787"},"source":["demo(before_tuning_lstm_classifier, device)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:582: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:775.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[0.6657010316848755,\n"," 0.6657010316848755,\n"," 0.6657010316848755,\n"," 0.6657010316848755,\n"," 0.6751184463500977]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NC6VkvfbnMgy","executionInfo":{"status":"ok","timestamp":1606112278140,"user_tz":480,"elapsed":580,"user":{"displayName":"J JS","photoUrl":"","userId":"03523663544813654809"}},"outputId":"0c107c90-0ca0-4ec6-987c-c95d7c7c042d"},"source":["demo(lstm_classifier, device)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.7254349589347839,\n"," 0.7254349589347839,\n"," 0.7254349589347839,\n"," 0.7254349589347839,\n"," 0.7893338799476624]"]},"metadata":{"tags":[]},"execution_count":15}]}]}